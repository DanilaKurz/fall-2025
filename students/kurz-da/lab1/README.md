# Лабораторная работа №1
В рамках первой лабораторной работы были выполнены все задания. 

1. *выбрать датасет для классификации, например на [kaggle](https://www.kaggle.com/datasets?&tags=13304-Clustering)*: был выбран датасет по классификации злокачественной/доброкачественной опухоли (рака груди). Был произведен первичный анализ, предобработка данных. Были выведены карты корреляций, удалены признаки, сильно коррелирующие между собой.

2. *реализовать вычисление отступа объекта (визуализировать, проанализировать)*: были вычислены отступы и создан график, который показывает насколько хорошо или плохо то или иное значение от истинной метки

3. *реализовать вычисление градиента функции потерь*: вообще, в качестве функции потерь была выбрана квадратичная функция. Просматривая лабораторную и слайды сейчас я вижу, что для бинарной классификации применяется бинарная функция потерь. Однако метрики оказались достойные. В конечном итоге я реализовал вычисление градиента функции потерь по цепному правилу и дальше использовал этот градиент во всех реализациях градиентного спуска. При этом мы учитывали отступ каждого элемента.

4. реализовать рекуррентную оценку функционала качества; - с этим случился провал, так как я изначально неправильно инициализировал Q как ноль. Хотя, вчитавшись в слайд, увидел, что инициализация происходит по случайной выборке, когда мы берем средний лосс по этим объектам. Следовательно, у меня не получились вразумительные графики и я их удалил. Тем не менее, реализовав функцию стохастического градиентного спуска, я нормально инициализировал Q, и дальше, уже понимая как применить и обновлять его внутри градиентного спуска, применил без учета инерции. Обучение через стохастический градиентный спуск и получившийся график рекуррентной ошибки выводятся в самом конце. В остальном коде закомментированы изменения Q

5. реализовать метод стохастического градиентного спуска с инерцией: Как оказывается, в первый раз я, видимо, пропустил "стохастического", тогда было бы проще. Как было написано в прошлом пункте, я реализовал функцию стохастического градиентного спуска (train_linear_classifier_sgd) без инерции. И в этой же я заменил шаг 6 и вместо градиентного шага без импульса, как это было изначально, реализовал изменение весов с инерцией

6. далее была реализована L2 регуляризация, был инициализирован гиперпараметр tau, заменен расчет градиента и модифицирован градиентный шаг.

7. далее был реализован скорейший градиентный спуск. Как указано в презентации, при квадратичной функции потерь h* = норма x_итого в минус второй. Сейчас я понимаю, что, скорее всего, (изначально) выполнил задание неправильно. Я применил новую переменную, прокомментировав это как параметр из метода Левенберга-Марквардта, имея в виду параметр, предотвращающий обнуление знаменателя, хотя это совершенно другой вариант выбора градиентного шага.
   Я переписал реализацию, используя как адаптивный шаг h* норму в минус второй, так как у нас квадратичная функция потерь.

8. Далее мы реализовали пример предъявления объектов по модулю отступа, взяв модуль каждого отступа, инициализировав их как начальные

9. Далее я написал функцию для обучения линейного классификатора - train_linear_classifier. Меняя инициализацию весов в соответствии с заданиями, я инициировал начальные веса через корреляцию Пирсона, получив по всем метрикам меньше 0.9. Затем был мультистарт, на котором выбил метрику в 0.941 recall, что является хорошим показателем на мед данных. После этого написал функцию для обучения через предъявление по отступам, то есть сначала шли наименее удачные предсказания модели в эпохе, от сложного к простому. Выбил так же 0.941 recall. Тут же вывел график лосса и увидел хорошее снижение уже к 3-4 эпохе.

10. Качество классификации было оценено в п.9, были рассчитаны метрики precision, recall, accuracy, f1, больше всего внимания уделяю именно полноте, так как важнее всего диагностировать злокачественность опухоли тогда, когда она действительно есть.

11. Реализация через sklearn с квадратичной функцией потерь, penalty=l2 получилась хуже по полноте, но лучше по точности, из чего можно сделать вывод, что я что-то сделал лучше.
