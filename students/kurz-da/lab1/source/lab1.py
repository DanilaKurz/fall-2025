# -*- coding: utf-8 -*-
"""notebook6bcf0b2bf3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/dankurz/notebook6bcf0b2bf3.c5e48c08-148d-446f-8c3e-0b18a46b5efd.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250917/auto/storage/goog4_request%26X-Goog-Date%3D20250917T201900Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2560c4e6894d0ba1e930a85a992806ed02ff7200d17776d88a72ce4e3af7a03f52dcdba445ffacbbde9a4114d3ee431e1bf14c7d6aeda1a837d19829cf2466c715e6dddb8eeca14a2f99ee2660ad21cf64a4bc2f424a8317807ce3adbce8e3194a499fcb1fbd77849c19984c362293cc207aeb5408af724de51e279406c7faa9c56bba97cdaa19f82410890797a66a7c438dc747362cd285cada14d292c37e68f67e46401da3c4fcd26af03ba03f229310c658a64035aa1f459660c430b1f923cfe69e0670b260eb48ef36fbc681bc3ef697c2e3ed7c298fdb47924d5bd761499e5e8ba94d2524f73abbd81551b8cc554eb04d63213e2a59dc012b920bce71d9
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

dankurz_cancer_data_path = kagglehub.dataset_download('dankurz/cancer-data')

print('Data source import complete.')

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

"""Для выполнения лабораторной работы взят датасет (https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)
Датасет содержит информацию о том, является ли опухоль доброкачественной или злокачественной. Вот описание признаков со страницы датасета:

~~~
Attribute Information:

1) ID number
2) Diagnosis (M = malignant, B = benign)
3-32)

Ten real-valued features are computed for each cell nucleus:

a) radius (mean of distances from center to points on the perimeter)
b) texture (standard deviation of gray-scale values)
c) perimeter
d) area
e) smoothness (local variation in radius lengths)
f) compactness (perimeter^2 / area - 1.0)
g) concavity (severity of concave portions of the contour)
h) concave points (number of concave portions of the contour)
i) symmetry
j) fractal dimension ("coastline approximation" - 1)

The mean, standard error and "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features. For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.

All feature values are recoded with four significant digits.

Missing attribute values: none

Class distribution: 357 benign, 212 malignant
~~~

"""

df = pd.read_csv('/kaggle/input/cancer-data/data.csv')
df.head(10)
# просто смотрим что внутри

df.columns  # смотрим какие есть колонки в датафрейме, чтобы что-нибудь удалить. видим 'Unnamed: 32' - удаляем, дропнем айди тоже
df = df.drop(columns=['id', 'Unnamed: 32'])

df.info()  # смотрим на пропуски, типы данных

"""Прямым текстом говорится, что для признака может существовать худшее, наибольшее, стандартная ошибка (среднее по трем наибольшим значениям).
Нам надо будет удалить сильно коррелирующие признаки и выбрать, на что мы будем смотреть - на среднее, минимум или максимум.

Выведем карту корреляций
"""

df['diagnosis'].unique()  # два класса, которые нам надо предсказать: M = malignant, B = benign (M доброкачественных, B злокачественных)
# переведем метки в нужный нам формат
df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': -1})

se_cols = [col for col in df.columns if '_se' in col]  # Не будем учитывать стандартное отклонение, оно не является показателем. Но посчитаем их чтобы определить, что мы ничего не потеряли
df_se = df[se_cols]
print(se_cols)

# разделим признаки на несколько датасетов, чтобы оценить, какие данные нам дадут более точные предсказания - mean или worst
worst_cols = [col for col in df.columns if '_worst' in col]
df_worst = df[worst_cols]
print(worst_cols)

mean_cols = [col for col in df.columns if '_mean' in col]
df_mean = df[mean_cols]
print(mean_cols)


print(len(df.columns) - (len(worst_cols) + len(mean_cols) + len(se_cols)))  # проверяем сколько колонок потерялось, только одна - диагноз

df_worst.info()

fig, axes = plt.subplots(1, 2, figsize=(16, 8))  # 1 строка, 3 столбца

# Функция для построения heatmap
def plot_corr_heatmap(data, title, ax):
    corr = data.corr()
    sns.heatmap(
        corr,
        annot=False,
        cmap='coolwarm',
        center=0,
        square=True,
        linewidths=0.5,
        cbar_kws={"shrink": 0.8},
        ax=ax
    )
    ax.set_title(title, fontsize=16, pad=20)
    ax.tick_params(axis='x', rotation=90)
    ax.tick_params(axis='y', rotation=0)

# Строим для каждого поднабора
plot_corr_heatmap(df_worst, 'Корреляции: _worst признаки', axes[0])
plot_corr_heatmap(df_mean, 'Корреляции: _mean признаки', axes[1])

plt.tight_layout()
plt.show()

"""По этим картам можем понять, что столбцы radius, perimetr, area максимально коррелируют. Скорее всего они являются просто разной интерпретацией одного показателя, поэтому оставим только радиус.

Достаточно сильно коррелируют concave points со многими колонками, а так же compactness с concavity.
compactness это (perimeter^2 / area - 1.0), (чем ближе к форме круга - тем более вероятна доброкачественность)
concavity - это вогнутость
concave points - это кол-во вогнутостей

Проведя небольшой анализ выяснил, что это одни из самых явных показателей злокачественной опухоли, поэтому их корреляция нормальна. Оставим их, избавившись только от area, perimetr
"""

df_worst = df_worst.drop(columns=['area_worst', 'perimeter_worst'])
df_mean = df_mean.drop(columns=['area_mean', 'perimeter_mean'])

df_worst.columns

df_worst.describe()  # смотрим в принципе всю доступную информацию по колонкам

import seaborn as sns
import matplotlib.pyplot as plt

# Исходные колонки
cols = df_worst.columns

# Удаляем radius_worst и texture_worst для первого графика
cols_filtered = cols.drop(['radius_worst', 'texture_worst'])

# Строим boxplot для всех ОСТАЛЬНЫХ колонок (без radius_worst и texture_worst)
plt.figure(figsize=(10, 4))
sns.boxplot(data=df_worst[cols_filtered], orient='h')
plt.title('Boxplot: Распределение признаков "worst" (без radius_worst и texture_worst)')
plt.xlabel('Значения')
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Строим гистограммы (оставим как есть, но тоже только по отфильтрованным колонкам)
fig, axes = plt.subplots(2, 4, figsize=(10, 4), dpi=100)
axes = axes.flatten()

for i, col in enumerate(cols_filtered):
    sns.histplot(df_worst[col], kde=True, ax=axes[i])
    axes[i].set_title(col)
    axes[i].set_xlabel('')


for j in range(len(cols_filtered), len(axes)):
    fig.delaxes(axes[j])

plt.suptitle('Гистограммы распределений (без radius_worst и texture_worst)', fontsize=14)
plt.tight_layout()
plt.show()

# Отдельный boxplot для radius_worst и texture_worst
special_cols = ['radius_worst', 'texture_worst']
plt.figure(figsize=(6, 3))
sns.boxplot(data=df_worst[special_cols])
plt.title('Boxplot: radius_worst и texture_worst')
plt.ylabel('Значения')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# приличное количество выбросов, но они не критически выступают, может быть информативно для модели
# начнем строить модель



y = df['diagnosis'].values
X_worst = df_worst.values
X_mean = df_mean.values

print(y.shape)  # выводим кол-во таргета - 569
print(X_worst.shape)  # выводим признаки - 569 строк на 8 колонок
print(X_mean.shape) # выводим признаки - 569 строк на 8 колонок

# нормализуем данные
mean = X_worst.mean(axis=0)  # среднее по каждому из 8 столбцов
std = X_worst.std(axis=0)    # стандартное отклонение по каждому столбцу

X_normalized = (X_worst - mean) / std

X_normalized

n_features_worst = X_normalized.shape[1]  # количество признаков
n_features_mean = X_mean.shape[1]
w_worst = np.random.randn(n_features_worst) * 0.001  # инициализируем веса, умножив их на 0.01, чтобы градиент сходился нормально
w_mean = np.random.randn(n_features_mean) * 0.001

print("Форма w:", w_worst.shape)
print("Форма w:", w_mean.shape)
print("Пример весов:", w_worst[:5])  # первые 5 весов

# выполним первое задание "реализовать вычисление отступа объекта (визуализировать, проанализировать);"
# отступ объекта - простыми словами - чем дальше от оси - тем лучше (или хуже)
# отступ - умножение значения дискриминантной функции g(x_итое, w) на истинную метку y_итое.

df_worst = df_worst.rename(columns={'concave points_worst': 'concave_points_worst'})  # в дальнейшем было замечено, что в выводе строки колонка записывается как "_5", поэтому произведем замену
df_mean = df_mean.rename(columns={'concave points_worst': 'concave_points_worst'})

margins = []
for i in range(len(X_normalized)):
    dot_of_vect_w = X_normalized[i] @ w_worst  # скалярное произведение вектора x_итое на веса
    margin = dot_of_vect_w * y[i]  # отступ - метка, умноженная на знак
    margins.append(margin)

margins = np.array(margins)

# так как будем визуализировать отступы для каждой вариации градиентного спуска - напишем функцию
# которая принимает список отступов, сортирует их и рисует график
def visualize_margins(margins):
    # Сортируем отступы
    sorted_margins = np.sort(margins)
    indices = np.arange(len(sorted_margins))

    plt.figure(figsize=(8, 4))

    # Создаем scatter plot ранжированных отступов
    plt.scatter(indices, sorted_margins,
               c=np.where(sorted_margins >= 0, 'green', 'red'),
               alpha=0.7, s=20, edgecolors='none')

    # Линия на нуле
    plt.axhline(y=0, color='black', linestyle='-', linewidth=2, label='нолевой отступ')

    plt.xlabel('Ранг объекта (по возрастанию отступа)', fontsize=12)
    plt.ylabel('Margin (отступ)', fontsize=12, fontweight='bold')
    plt.title('Ранжирование объектов по отступам', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.ylim(-1.1, 2.3)
    plt.tight_layout()
    plt.show()

visualize_margins(margins)

# мы построили график ранжирования отступов, но понятно, что со случайными весами результат будет случайный, что мы и видим. в дальнейшем построим его еще раз

# Задание 2 - реализовать вычисление градиента функции потерь;
# градиент - на сколько надо изменить веса, чтобы уменьшить ошибку, это вектор частных производных функции по каждому параметру.
# функция потерь - лосс - квадратичная функция потерь для линейного классификатора по отступу

# то есть 1 - M_итое, возведенное в квадрат (чтобы ошибка всегда была положительная)
# градиент в нашем случае будет вектором частных производных функции по каждому весу

#  "L(w)=(1−y⋅⟨x,w⟩)^2"

# тут вроде как надо посчитать по цепному правилу, где ⟨x,w⟩ = z, M=y⋅z, s=1−M, L=s^2
# умножаем производные по каждому слою - то есть ∂L/∂s * ∂s/∂M * ∂M/∂z * ∂z/∂w_житое (или )
# 1. ∂L/∂s = 2s, 2s = 2 * (1 - M)
# 2. ∂s/∂M = производная разности по отступу M: производная разности (1 - M) равно разности производных. 1 - константа, ее производная = 0, производная M = 1 (потому что M^0 = 1), итого = -1
# 3. ∂M/∂z = производная (y⋅z) по z, то есть ∂(y*z)/∂z = y, потому что при умножении функции на константу, можно вынести константу за производную. Производная z = 1, y * 1 = y
# 4. ∂z/∂w_какое-то (житое скорее всего) = производная Скалярного произведения по весу w_i = x_i (при дифференцировании по w_итое все другие слагаемые x_j w_j (где j != i) считаются константами и обращаются в ноль )

# В итоге мы все это перемножаем: 2 * (1 - M) * -1 * y * x_итое
# Следовательно, градиент квадратичной функции потерь равен -2 * y * (1 - M) * x

grads = []  # инициализация списка для градиента
for i in range(len(X_normalized)):
    grad = -2 * y[i] * (1 - margins[i]) * X_normalized[i]  # 2 * (1 - M) * -1 * y * x_итое
    # print(grad)
    grads.append(grad)

total_gradient = np.sum(grads, axis=0)
print(total_gradient)  # мы выполнили задание и нашли градиент квадратичной функции потерь по отступу

# Дальше идет реализация градиентного спуска с инерцией с рекуррентной оценкой кач-ва
# Нужно инициализировать три гиперпараметра - скорость обучения, темп забывания и параметр экспоненциального скользящего среднего

h = 0.001  # скорость обучения
lambd = 0.1  # темп забывания
gamma = 0.9  # параметр экспоненциального скользящего среднего (коэф. инерции)
v = np.zeros(len(w_worst))  # импульс
Q_avg = 0.0  # рекуррентная оценка ошибки
history = []  # хранение истории изменения ошибки на каждом шаге
loss_history = []

margins = []
indices = np.arange(len(X_normalized))  # индексы
np.random.shuffle(indices)
for i in indices:
    x_i = X_normalized[i]  # случайное значение выборки
    y_i = y[i]  # правильный знак этого вектора
    margin = y_i * (x_i @ w_worst)
    margins.append(margin)
    loss = (1 - margin) ** 2
    loss_history.append(loss)
    grad = -2 * (1 - margin) * y_i * x_i

    v = gamma * v + (1 - gamma) * grad  # обновление импульса на каждом шаге
    w_worst = w_worst - h * v  # обновление весов с учетом скорости обучения и импульса
    # Q_avg = lambd * loss + (1 - lambd) * Q_avg  # Экспоненциальное скользящее среднее
    # history.append(Q_avg)

Q = np.mean(history)  # то, что написано выше, может и будет работать только после нормальной инициализации оценки функционала





visualize_margins(margins)

# и это все неправильно. реализую рекуррентную оценку в стохастическом градиенте

v = np.zeros(len(w_worst))  # импульс

def train_linear_classifier_sgd(X_train, y_train, w_init, h=0.001, lambd=0.02, tau=0.01, n_iter=1000):
    """
    Стохастический градиентный спуск по алгоритму со слайда
    """
    w = w_init.copy()
    n_samples = len(X_train)

    # Шаг 2: Инициализация Q
    random_indices = np.random.choice(n_samples, size=min(20, n_samples), replace=False)
    initial_losses = []
    for i in random_indices:
        x_i = X_train[i]
        y_i = y_train[i]
        margin = y_i * (x_i @ w)
        loss = ((1 - margin) ** 2) + (tau/2) * np.sum(w**2)
        initial_losses.append(loss)

    Q = np.mean(initial_losses)

    # История для отслеживания
    Q_history = [Q]
    loss_history = []

    # Шаги 3-8: Основной цикл
    for iteration in range(n_iter):
        # Шаг 4: Случайный объект (стохастичность!)
        i = np.random.randint(n_samples)
        x_i = X_train[i]
        y_i = y_train[i]

        # Шаг 5: Вычислить потерю
        margin = y_i * (x_i @ w)
        loss = ((1 - margin) ** 2) + (tau/2) * np.sum(w**2)
        loss_history.append(loss)

        grad = -2 * (1 - margin) * y_i * x_i + tau * w

        # Шаг 6: Градиентный шаг (БЕЗ импульса!)
        w = w - h * grad

        # Шаг 6: ГРАДИЕНТНЫЙ ШАГ С ИНЕРЦИЕЙ
        v = gamma * v + (1 - gamma) * grad
        w = w - h * v


        # Шаг 7: Оценить функционал
        Q = lambd * loss + (1 - lambd) * Q
        Q_history.append(Q)

        # Мониторинг сходимости
        if iteration % 100 == 0:
            print(f'Iteration {iteration}: Q = {Q:.4f}, Loss = {loss:.4f}')

    return w, Q_history, loss_history

# я понял, что рекуррентная оценка инициализируется средним лоссом по случайному подмножеству (есть на слайде, реализовано в функции).
# а дальше обновляется именно так, как и обновлялось у меня, но в стохастическом град спуске, без импульса

# начнем делать L2 регуляризацию, для этого сначала попробуем инициализировать переменную tau со значением 0.01
tau = 0.01
history = []

margins = []
indices = np.arange(len(X_normalized))  # индексы
np.random.shuffle(indices)
for i in indices:
    x_i = X_normalized[i]  # случайное значение выборки
    y_i = y[i]  # правильный знак этого вектора
    margin = y_i * (x_i @ w_worst)
    margins.append(margin)
    loss = ((1 - margin) ** 2) + (tau/2) * sum([w_j**2 for w_j in w_worst])  # по формуле мы берем норму в квадрате, то есть сумму весов (w**2)
    grad = -2 * (1 - margin) * y_i * x_i + tau * w_worst  # градиент с примененной регуляризацией

    v = gamma * v + (1 - gamma) * grad  # обновление импульса на каждом шаге
    w_worst = w_worst - h * v  # обновление весов с учетом скорости обучения и импульса
    # Q_avg = lambd * loss + (1 - lambd) * Q_avg  # Экспоненциальное скользящее среднее
    # history.append(Q_avg)

margins = np.array(margins)

visualize_margins(margins)

# мы визуализировали отступы еще раз и теперь видим, отступы не болтаются в середине, наблюдается стабильный рост предсказательной способности модели.

# следующее задание - "реализовать скорейший градиентный спуск;"
# скорейший градиентный спуск подразумевает нахождение такого шага h, который минимизирует функцию потерь для данного примера
# при квардратичной функции потерь шаг h = норма вектора x_итое в минус второй степени, то есть (1 / x_итое**2)

# в ходе выполнения работы была замечена ошибка - я не использовал адаптивный шаг h_opt, вместо него использовал h,
# оказалось, что просто подставить h_opt нельзя, так как он не работает с регуляризацией и импульсом (но возможно я не правильно понял)

history = []
margins = []
indices = np.arange(len(X_normalized))  # индексы
np.random.shuffle(indices)
h = 0.01
tau = 0.001
lambd = 0.1  # темп забывания
gamma = 0.9
for i in indices:
    x_i = X_normalized[i]  # случайное значение выборки
    y_i = y[i]  # правильный знак этого вектора
    margin = y_i * (x_i @ w_worst)
    margins.append(margin)
    loss = ((1 - margin) ** 2) + (tau/2) * sum([w_j**2 for w_j in w_worst])  # по формуле мы берем норму в квадрате, то есть сумму весов (w**2)
    grad = -2 * (1 - margin) * y_i * x_i  # градиент БЕЗ регуляризации

    mu = 1e-8  # параметр из метода Левенберга-Марквардта
    norm_squared = np.linalg.norm(x_i) ** 2  # можно использовать встроенные методы нумпая
    h_opt = 0.5 / (norm_squared + mu)
    h_opt = h_opt * 0.01  # масштабируем
    h_opt = min(h_opt, 0.1)
    w_worst = w_worst - h_opt * grad  # обновление весов БЕЗ ИМПУЛЬСА
    # Q_avg = lambd * loss + (1 - lambd) * Q_avg  # можно забыть, так как я использовал это неправильно, но как назидание оставлю
    # history.append(Q_avg)
    # print(f"||x_i||² = {norm_squared:.3f}, h_opt = {h_opt:.3f}")

# таким образом мы реализовали скорейший градиентный спуск. (И ЭТО НЕПРАВИЛЬНОЕ РЕШЕНИЕ)
# я как мог подбирал параметры, может быть не до конца понял, почему скорейший градиентный спуск работает немного по другому (потому что это неправильное решение)

# сейчас реализуем правильное решение

indices = np.arange(len(X_normalized))  # индексы
np.random.shuffle(indices)
h = 0.01
tau = 0.001
lambd = 0.1  # темп забывания
gamma = 0.9
for i in indices:
    x_i = X_normalized[i]  # случайное значение выборки
    y_i = y[i]  # правильный знак этого вектора
    margin = y_i * (x_i @ w_worst)
    margins.append(margin)
    loss = ((1 - margin) ** 2) + (tau/2) * sum([w_j**2 for w_j in w_worst])  # по формуле мы берем норму в квадрате, то есть сумму весов (w**2)
    grad = -2 * (1 - margin) * y_i * x_i  # градиент БЕЗ регуляризации

    norm_squared = np.linalg.norm(x_i) ** -2  # можно использовать встроенные методы нумпая
    w_worst = w_worst - norm_squared * grad  # обновление весов БЕЗ ИМПУЛЬСА

# опять же, из презентации, норма в минус второй является оптимальной скоростью обучения, адаптивным шагом

# следующее задание - реализовать предъявление объектов мо модулю отступа;
# то есть раз в какое-то время сортировать объекты в порядке уверенности модели в предсказании
# сделаем раз в эпоху - каждая новая эпоха будет инициализирована не шафлом, а отступами в порядке возрастания уверенности
# приведем пример того как это можно сделать, начиная с нулевой эпохи

history = []
margins = []  # список, в который будуд собираться отступы на каждой итерации
indices = np.arange(len(X_normalized))  # индексы
margins_start = [abs(y_i * (x_i @ w_worst)) for x_i, y_i in zip(X_normalized, y)]  # инициируем первые отступы
w = w_worst.copy()
sorted_indices = np.argsort(margins_start)  # сортируем первые отступы
for i in sorted_indices:
    x_i = X_normalized[i]  # идем по порядку выборки
    y_i = y[i]  # правильный знак этого вектора
    margin = y_i * (x_i @ w)
    margins.append(margin)
    loss = ((1 - margin) ** 2) + (tau/2) * sum([w_j**2 for w_j in w])  # по формуле мы берем норму в квадрате, то есть сумму весов (w**2)
    grad = -2 * (1 - margin) * y_i * x_i + tau * w  # градиент с примененной регуляризацией

    v = gamma * v + (1 - gamma) * grad  # обновление импульса на каждом шаге
    w = w - h * v  # обновление весов с учетом скорости обучения и импульса
    # Q_avg = lambd * loss + (1 - lambd) * Q_avg  # Экспоненциальное скользящее среднее
    # history.append(Q_avg)

print(history[:10])
print(history[-10:])

visualize_margins(margins)

# видим увеличение отступов в пользу предсказательной способности

# Сначала перемешаем данные, чтобы разделение было случайным
indices = np.arange(len(X_normalized))
np.random.shuffle(indices)

# Разделяем индексы на train (70%) и test (30%)
split_idx = int(0.7 * len(X_normalized))
train_indices = indices[:split_idx]
test_indices = indices[split_idx:]

# Создаем train и test выборки
X_train = X_normalized[train_indices]
y_train = y[train_indices]
X_test = X_normalized[test_indices]
y_test = y[test_indices]

print(f"Train: {len(X_train)} samples")
print(f"Test: {len(X_test)} samples")

# обернем линейный классификатор в функцию
# h = 0.001  # скорость обучения
# lambd = 0.1  # темп забывания
# gamma = 0.9  # параметр экспоненциального скользящего среднего (коэф. инерции)
# v = np.zeros(len(w_worst))  # импульс
# Q_avg = 0  # рекуррентная оценка ошибки (опять же, забудем, что она есть)
# history = []  # хранение истории изменения ошибки на каждом шаге


def train_linear_classifier(X_train, y_train, w_init, h=0.001, lambd=0.1, gamma=0.9, tau=0.01, epochs=50):
    """
    Обучение с квадратичной функцией потерь
    X_train - обучающие данные
    y_train - метки
    w_init - начальные веса
    h - скорость обучения
    tau - коэффициент регуляризации
    epochs - количество эпох
    """
    w = w_init.copy()
    n_samples = len(X_train)
    train_history = []
    Q_history_epochs = []
    v = np.zeros(len(w_init))
    Q_avg = 0

    for epoch in range(epochs):
        history = []
        # Перемешиваем данные в каждой эпохе
        indices = np.random.permutation(n_samples)
        epoch_loss = 0

        for i in indices:
            x_i = X_train[i]
            y_i = y_train[i]

            margin = y_i * (x_i @ w)
            loss = ((1 - margin) ** 2) + (tau/2) * sum([w_j**2 for w_j in w])
            epoch_loss += loss


            grad = -2 * (1 - margin) * y_i * x_i + tau * w  # Вычисляем градиент
            v = gamma * v + (1 - gamma) * grad  # импульс
            w = w - h * v  # Градиентный шаг
            # Q_avg = lambd * loss + (1 - lambd) * Q_avg  # Экспоненциальное скользящее среднее
            # history.append(Q_avg)

        Q_history_epochs.append(sum(history)/len(indices))  # чтобы посмотреть по эпохам скользящее среднее

        train_history.append(epoch_loss / n_samples)  # Сохраняем средние потери за эпоху

    return w, train_history, Q_history_epochs

def correlation_initialization(X, y):
    """
    Инициализация весов через корреляцию с метками
    X - матрица признаков (n_samples, n_features)
    y - вектор меток
    """
    n_features = X.shape[1]
    w_init = np.zeros(n_features)

    for j in range(n_features):
        # Вычисляем корреляцию j-го признака с меткой y
        feature_j = X[:, j]
        correlation = np.corrcoef(feature_j, y)[0, 1]  # коэффициент корреляции Пирсона

        # Если корреляция NaN (например, если признак постоянный), ставим 0
        if not np.isnan(correlation):
            w_init[j] = correlation
        else:
            w_init[j] = 0

    return w_init

# Пример использования
w_corr = correlation_initialization(X_train, y_train)
print("Веса после корреляционной инициализации:", w_corr)

w_final, train_history, Q_history_epochs = train_linear_classifier(X_train, y_train, w_corr)  # используем найденную корреляцию как стартовые веса

w_final

def predict(X, w):
    """Предсказание меток по весам"""
    scores = X @ w  # линейная комбинация
    return np.sign(scores)  # возвращаем знак: +1 или -1


def calculate_metrics(X, y, w, positive_class=1):
    """
    Вычисление метрик классификации
    """
    predictions = predict(X, w)

    # True Positive: правильно предсказанные положительные
    tp = np.sum((predictions == positive_class) & (y == positive_class))

    # True Negative
    tn = np.sum((predictions != positive_class) & (y != positive_class))

    # False Positive: отрицательные, ошибочно предсказанные как положительные
    fp = np.sum((predictions == positive_class) & (y != positive_class))

    # False Negative: положительные, ошибочно предсказанные как отрицательные
    fn = np.sum((predictions != positive_class) & (y == positive_class))

    # Вычисляем метрики
    accuracy = (tp + tn) / len(y)  # достоверность
    precision = tp / (tp + fp)  # точность
    recall = tp / (tp + fn)  # полнота
    f1 = 2 * (precision * recall) / (precision + recall)

    return accuracy, precision, recall, f1

# Проверяем на тесте по предъявлению через корреляцию
test_accuracy, test_precision, test_recall, test_f1 = calculate_metrics(X_test, y_test, w_corr)

print(f"Accuracy на тесте: {test_accuracy:.3f}")
print(f"Precision на тесте: {test_precision:.3f}")
print(f"Recall на тесте: {test_recall:.3f}")
print(f"f1 на тесте: {test_f1:.3f}")

# мы видим относительно неплохую полноту, которой стоит отдавать большее предпочтение на медицинских данных

# следующее обучение - через мультистарт со случайными весами

def multistart_training(X_train, y_train, X_test, y_test, n_restarts=5, epochs=50):
    """
    Мультистарт: несколько запусков со случайными начальными весами
    """
    best_w = None
    best_test_accuracy = 0
    best_results = None
    n_features = X_train.shape[1]

    all_results = []

    for restart in range(n_restarts):
        print(f"Запуск {restart + 1}/{n_restarts}")

        # Случайная инициализация весов
        w_init = np.random.randn(n_features) * 0.01

        # Обучаем модель
        w_trained, train_history, Q_history_epochs = train_linear_classifier(X_train, y_train, w_init, epochs=epochs)

        # Проверяем на тесте
        test_accuracy, test_precision, test_recall, test_f1 = calculate_metrics(X_test, y_test, w_trained)

        # Сохраняем результаты
        results = {
            'weights': w_trained,
            'test_accuracy': test_accuracy,
            'test_precision': test_precision,
            'test_recall': test_recall,
            'test_f1': test_f1,
            'train_history': train_history
        }

        all_results.append(results)

        # Обновляем лучший результат
        if test_accuracy > best_test_accuracy:
            best_test_accuracy = test_accuracy
            best_w = w_trained
            best_results = results

    return best_w, best_results, all_results

# Запускаем мультистарт
best_w_multistart, best_results, all_results = multistart_training(
    X_train, y_train, X_test, y_test, n_restarts=5, epochs=50
)

print(f"\nЛучший результат мультистарта:")
print(f"Достоверность: {best_results['test_accuracy']:.3f}")
print(f"Точность: {best_results['test_precision']:.3f}")
print(f"Полнота: {best_results['test_recall']:.3f}")
print(f"f1: {best_results['test_f1']:.3f}")

# Отличные результаты через мультистарт
# Теперь обучим со случайным предъявлением и предъявлением по отступам

def train_linear_classifier_margins(X_train, y_train, w_init, h=0.001, lambd=0.02, gamma=0.9, tau=0.01, epochs=50):
    """
    Обучение с квадратичной функцией потерь
    X_train - обучающие данные
    y_train - метки
    w_init - начальные веса
    h - скорость обучения
    tau - коэффициент регуляризации
    epochs - количество эпох
    """
    w = w_init.copy()
    n_samples = len(X_train)
    v = np.zeros_like(w_init)
    train_history = []
    Q_avg = 0
    history = []


    # indices = np.arange(len(X_normalized))  # индексы
    margins_start = [abs(y_i * (x_i @ w_init)) for x_i, y_i in zip(X_train, y_train)]  # инициируем первые отступы
    sorted_indices = np.argsort(margins_start)

    for epoch in range(epochs):
        # Перемешиваем данные в каждой эпохе
        # indices = np.random.permutation(n_samples)
        margins = []  # список, в который будуд собираться отступы на каждой итерации
        epoch_loss = 0

        for i in sorted_indices:
            x_i = X_train[i]
            y_i = y_train[i]

            margin = y_i * (x_i @ w)
            margins.append(margin)
            loss = ((1 - margin) ** 2) + (tau/2) * sum([w_j**2 for w_j in w])
            epoch_loss += loss


            grad = -2 * (1 - margin) * y_i * x_i + tau * w  # Вычисляем градиент
            v = gamma * v + (1 - gamma) * grad  # импульс
            w = w - h * v  # Градиентный шаг
            Q_avg = lambd * loss + (1 - lambd) * Q_avg  # Экспоненциальное скользящее среднее
            history.append(Q_avg)

        sorted_indices = np.argsort(margins)

        train_history.append(epoch_loss / n_samples)  # Сохраняем средние потери за эпоху

    return w, train_history, margins


w = np.random.randn(X_train.shape[1]) * 0.001
w_marg, train_history, margins = train_linear_classifier_margins(X_train, y_train, w)
test_accuracy, test_precision, test_recall, test_f1 = calculate_metrics(X_test, y_test, w_marg)

print(f"Accuracy на тесте: {test_accuracy:.3f}")
print(f"Precision на тесте: {test_precision:.3f}")
print(f"Recall на тесте: {test_recall:.3f}")
print(f"f1 на тесте: {test_f1:.3f}")

plt.plot(train_history)  # вот потери на эпоху образуют шикарный график

visualize_margins(margins)

from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

epochs = 50
# Обучаем sklearn SGD (эталон)
clf = SGDClassifier(loss='squared_error', penalty='l2', alpha=tau, learning_rate='constant', eta0=h, max_iter=epochs, random_state=42)
clf.fit(X_train, y_train)

# Предсказания
preds_sklearn = clf.predict(X_test)

# Метрики sklearn
print("Эталон (sklearn):")
print(f"Accuracy: {accuracy_score(y_test, preds_sklearn):.4f}")
print(f"Precision: {precision_score(y_test, preds_sklearn, pos_label=1):.4f}")
print(f"Recall: {recall_score(y_test, preds_sklearn, pos_label=1):.4f}")
print(f"F1: {f1_score(y_test, preds_sklearn, pos_label=1):.4f}")

# сравнивая с эталонным результатом, мы видим, что модель правильно определила все случаи, когда опухоль была злокачественной
# тем не менее, полнота страдает, всего 89% всех данных были определены правильно, что плохо для медицинских данных
# тем временем наш лучший результат полноты - 96%!

w = np.random.randn(X_train.shape[1]) * 0.001
w_marg, train_history, margins = train_linear_classifier_sgd(X_train, y_train, w)

plt.plot(train_history)

